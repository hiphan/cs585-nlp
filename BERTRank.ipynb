{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOkgtquc2ziK",
        "colab_type": "text"
      },
      "source": [
        "Google Colab link: https://colab.research.google.com/drive/1PLyNxB430viZId2-pEFFNWUkYfYsv2s9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvrNN4u07int",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modifications copyright (C) 2013 Hieu Phan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "#Document Classification by Reading Difficulty with BERT #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "source": [
        "Import the neccessary libraries.  \n",
        "Note: the BERT library below only works with Tensorflow version under 2.0.0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package from Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "9f757e4d-f0e0-40c0-b50f-9c51b6e1ed60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "outputId": "59d05583-73d4-40b9-afaf-731ec220ec3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "52df12c2-01e8-4be4-ef46-90a7e58af175",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JriF_RpT96o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = F\"/content/gdrive/My Drive/585/\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "source": [
        "Load the Newsela dataset from the mounted drive. These two datasets are generated from the Newsela corpus using stratified sampling with training/test ratio of 9:1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# Load both training and test set from Google Drive into DataFrames.\n",
        "def load_dataset(path='/content/gdrive/My Drive/585/'):\n",
        "  training = pd.read_csv(path + 'training_newsela.csv')\n",
        "  test = pd.read_csv(path + 'test_newsela.csv')\n",
        "  return training, test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "source": [
        "Print out the sizes of our training and test sets and also an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "outputId": "ad0b6cfe-c6b5-4ccd-cb8c-485397c447e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Training set contains %d examples' % len(train))\n",
        "print('Test set contains %d examples' % len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set contains 8570 examples\n",
            "Test set contains 953 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIz-bzm_jiB",
        "colab_type": "code",
        "outputId": "1d15eb69-53d2-4082-a420-03b1a15516ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "idx = np.random.randint(0, len(train))\n",
        "print('Example from the training set:')\n",
        "print('Text:' + train.iloc[idx].text)\n",
        "print('Difficulty (between 0 and 4): %d' % train.iloc[idx].label) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example from the training set:\n",
            "Text:Maricopa County Sheriff Joe Arpaio calls himself \"the toughest sheriff in America.\" The Arizona lawman has made national headlines multiple times. He faces lawsuits from the U.S. Justice Department and private citizens for allegedly singling out Latinos, a practice known as racial profiling. He launched an investigation into President Obama's birth certificate, and he published a biography called \"Joe's Law\" that included some controversial statements about Mexican-Americans.\n",
            "\n",
            "Following the Sandy Hook Elementary School tragedy, officials all over the country are trying to figure out the best way to keep schools safe. Last week, Sheriff Arpaio put his own new school safety plan into action. He sent out his volunteer posse to patrol 59 schools in Maricopa County.\n",
            "\n",
            "The posse is a group the sheriff assembled in 1993 to help with shopping mall thefts. Now, it is best known for its role in assisting the sheriff's workplace raids. The raids targeted immigrants who did not have documents to work in the U.S.\n",
            "\n",
            "## Who Called The Posse?\n",
            "\n",
            "Posse volunteers drive around school grounds looking for suspicious activity. They are not inside schools or even on school property. A lieutenant commander in the posse, Dennis Donowick, told The New York Times that when they are on patrol, posse volunteers look for someone they feel does not belong, such as \"a guy in (a) trench coat in the middle of summer.\"\n",
            "\n",
            "Most of the schools the posse currently patrols are in small, rural places the sheriff's office was already policing. School officials and parents were not asked if they wanted the volunteers.\n",
            "\n",
            "Ann Donahue, a district spokeswoman, said that school officials heard of the sheriff's plans from the news, not from his office. She said school officials do not necessarily oppose the new plan, but that there are details that schools need to consider.\n",
            "\n",
            "Sheriff Arpaio did not need extra money to finance the program because posse members do not get paid for working. They receive some basic training from the sheriff's department. Some volunteers are armed with automatic or semi-automatic weapons.\n",
            "\n",
            "## Critics Question The Plan\n",
            "\n",
            "The plan has its critics. Some wonder about the background of the volunteers and if they can be trusted. The posse includes military veterans and former police officers. However, a local television news investigation revealed that some posse members have criminal records, including assault and disorderly conduct charges.\n",
            "\n",
            "Arizona State Representative Chad Campbell opposes the sheriff's plan. He has stated that he was not sure how effective officers could be by simply driving around schools and not interacting with administrators or teachers.\n",
            "\n",
            "The sheriff says that publicity about the program will help prevent violent criminals from entering schools.\n",
            "\n",
            "\"This will be mainly a deterrent, prevention, that's why we're sending marked cars out there,\" Arpaio told The Arizona Republic. \"But if we need to take action, we will.\"\n",
            "\n",
            "## Armed Guards For Every School\n",
            "\n",
            "The move by Sheriff Arpaio comes in a highly charged political climate. When President Obama made his new gun control plan public, he was surrounded by children who had written to him about gun violence. He vowed to convince the public to support better gun control laws. The plan would require universal background checks and would ban assault weapons. These are measures the National Rifle Association (NRA) opposes.\n",
            "\n",
            "Instead, the NRA has called for armed police officers in every school in the nation. A new NRA ad aims to persuade the public to oppose Obama's plan. It asks, \"Are the president's kids more important than yours? Then why is he skeptical about putting armed security in our schools when his kids are protected by armed guards at their school.\"\n",
            "\n",
            "The White House called the NRA video \"repugnant and cowardly.\" Jay Carney, Obama's press secretary, said most Americans agree that a president's children should not be used as \"pawns in a political fight.\" Secret Service agents protect the president's daughters. Their school has no armed guards on staff.\n",
            "\n",
            "Sheriff Arpaio has said that he supports the NRA's plan for an armed officer in every school. He also says that his plan differs from the NRA's proposal. Its focus is on enhancing security in the area surrounding schools, not directly inside them.\n",
            "\n",
            "The sheriff first spoke about his plan in December, shortly after the Sandy Hook Elementary School tragedy. He told ABC/Univision in December that his plan was not up for debate. \"It doesn't matter whether they like it or don't. I'm still going to do it. I can't imagine criticism coming when they're given free protection.\"\n",
            "Difficulty (between 0 and 4): 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab_type": "code",
        "outputId": "e46afd28-2043-4713-ec60-5b53083e9d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'text', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0, 1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "78b3a56a-7a44-4649-e3d2-e93139d281c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "source": [
        "Tokenized example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Convert to features for BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "644f1189-b7fc-4403-d98a-e3fa34b1a0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 8570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 8570\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lo ##me , togo — soccer officials in the african country of togo are afraid to let their team travel to guinea for a match . guinea is where the outbreak of the deadly e ##bola disease started . to protect their players , togo is asking that the match be moved to another country . the soccer match is part of the african cup . it is the biggest soccer tournament in africa and it happens every year . soccer officials fear the spread of the painful and con ##tag ##ious disease could ruin the african cup ' s final qualifying round . the winners of the qualifying round will get to play in the finals in morocco . e ##bola has killed nearly 1 [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lo ##me , togo — soccer officials in the african country of togo are afraid to let their team travel to guinea for a match . guinea is where the outbreak of the deadly e ##bola disease started . to protect their players , togo is asking that the match be moved to another country . the soccer match is part of the african cup . it is the biggest soccer tournament in africa and it happens every year . soccer officials fear the spread of the painful and con ##tag ##ious disease could ruin the african cup ' s final qualifying round . the winners of the qualifying round will get to play in the finals in morocco . e ##bola has killed nearly 1 [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8840 4168 1010 23588 1517 4715 4584 1999 1996 3060 2406 1997 23588 2024 4452 2000 2292 2037 2136 3604 2000 7102 2005 1037 2674 1012 7102 2003 2073 1996 8293 1997 1996 9252 1041 24290 4295 2318 1012 2000 4047 2037 2867 1010 23588 2003 4851 2008 1996 2674 2022 2333 2000 2178 2406 1012 1996 4715 2674 2003 2112 1997 1996 3060 2452 1012 2009 2003 1996 5221 4715 2977 1999 3088 1998 2009 6433 2296 2095 1012 4715 4584 3571 1996 3659 1997 1996 9145 1998 9530 15900 6313 4295 2071 10083 1996 3060 2452 1005 1055 2345 6042 2461 1012 1996 4791 1997 1996 6042 2461 2097 2131 2000 2377 1999 1996 4399 1999 9835 1012 1041 24290 2038 2730 3053 1015 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8840 4168 1010 23588 1517 4715 4584 1999 1996 3060 2406 1997 23588 2024 4452 2000 2292 2037 2136 3604 2000 7102 2005 1037 2674 1012 7102 2003 2073 1996 8293 1997 1996 9252 1041 24290 4295 2318 1012 2000 4047 2037 2867 1010 23588 2003 4851 2008 1996 2674 2022 2333 2000 2178 2406 1012 1996 4715 2674 2003 2112 1997 1996 3060 2452 1012 2009 2003 1996 5221 4715 2977 1999 3088 1998 2009 6433 2296 2095 1012 4715 4584 3571 1996 3659 1997 1996 9145 1998 9530 15900 6313 4295 2071 10083 1996 3060 2452 1005 1055 2345 6042 2461 1012 1996 4791 1997 1996 6042 2461 2097 2131 2000 2377 1999 1996 4399 1999 9835 1012 1041 24290 2038 2730 3053 1015 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] los angeles — the sar ##dine fishing boat eileen glided through moon ##lit waters as it traveled from san pedro to santa catalina island off the california coast . its tired - eyed captain had grown more desperate as the evening wore on . but after 12 hours and $ 1 , 000 worth of fuel , corbin hanson and his crew returned to port . the hadn ' t caught a single small , silvery sar ##dine . \" tonight ' s pretty reflective of how things have been going , \" hanson said . \" not very well . \" to blame is the biggest sar ##dine crash in generations , which has made schools of the fish rare on the west coast . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] los angeles — the sar ##dine fishing boat eileen glided through moon ##lit waters as it traveled from san pedro to santa catalina island off the california coast . its tired - eyed captain had grown more desperate as the evening wore on . but after 12 hours and $ 1 , 000 worth of fuel , corbin hanson and his crew returned to port . the hadn ' t caught a single small , silvery sar ##dine . \" tonight ' s pretty reflective of how things have been going , \" hanson said . \" not very well . \" to blame is the biggest sar ##dine crash in generations , which has made schools of the fish rare on the west coast . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3050 3349 1517 1996 18906 10672 5645 4049 20495 26936 2083 4231 15909 5380 2004 2009 6158 2013 2624 7707 2000 4203 22326 2479 2125 1996 2662 3023 1012 2049 5458 1011 7168 2952 2018 4961 2062 7143 2004 1996 3944 5078 2006 1012 2021 2044 2260 2847 1998 1002 1015 1010 2199 4276 1997 4762 1010 24003 17179 1998 2010 3626 2513 2000 3417 1012 1996 2910 1005 1056 3236 1037 2309 2235 1010 21666 18906 10672 1012 1000 3892 1005 1055 3492 21346 1997 2129 2477 2031 2042 2183 1010 1000 17179 2056 1012 1000 2025 2200 2092 1012 1000 2000 7499 2003 1996 5221 18906 10672 5823 1999 8213 1010 2029 2038 2081 2816 1997 1996 3869 4678 2006 1996 2225 3023 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3050 3349 1517 1996 18906 10672 5645 4049 20495 26936 2083 4231 15909 5380 2004 2009 6158 2013 2624 7707 2000 4203 22326 2479 2125 1996 2662 3023 1012 2049 5458 1011 7168 2952 2018 4961 2062 7143 2004 1996 3944 5078 2006 1012 2021 2044 2260 2847 1998 1002 1015 1010 2199 4276 1997 4762 1010 24003 17179 1998 2010 3626 2513 2000 3417 1012 1996 2910 1005 1056 3236 1037 2309 2235 1010 21666 18906 10672 1012 1000 3892 1005 1055 3492 21346 1997 2129 2477 2031 2042 2183 1010 1000 17179 2056 1012 1000 2025 2200 2092 1012 1000 2000 7499 2003 1996 5221 18906 10672 5823 1999 8213 1010 2029 2038 2081 2816 1997 1996 3869 4678 2006 1996 2225 3023 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] nasa ' s newest robotic explorer rocket ##ed into space late friday from virginia , dazzling sky watch ##ers along the east coast . but the lad ##ee spacecraft quickly ran into equipment trouble , and while nasa assured everyone early saturday that the lunar probe was safe and on a perfect track for the moon , officials acknowledged the problem needs to be resolved in the next two to three weeks . s . peter word ##en , director of nasa ' s ames research center in california , told reporters he ' s confident everything will be working properly in the next few days . the spacecraft was developed at ames . lad ##ee ' s reaction wheels were turned on to orient and [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] nasa ' s newest robotic explorer rocket ##ed into space late friday from virginia , dazzling sky watch ##ers along the east coast . but the lad ##ee spacecraft quickly ran into equipment trouble , and while nasa assured everyone early saturday that the lunar probe was safe and on a perfect track for the moon , officials acknowledged the problem needs to be resolved in the next two to three weeks . s . peter word ##en , director of nasa ' s ames research center in california , told reporters he ' s confident everything will be working properly in the next few days . the spacecraft was developed at ames . lad ##ee ' s reaction wheels were turned on to orient and [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9274 1005 1055 14751 20478 10566 7596 2098 2046 2686 2397 5958 2013 3448 1010 28190 3712 3422 2545 2247 1996 2264 3023 1012 2021 1996 14804 4402 12076 2855 2743 2046 3941 4390 1010 1998 2096 9274 8916 3071 2220 5095 2008 1996 11926 15113 2001 3647 1998 2006 1037 3819 2650 2005 1996 4231 1010 4584 8969 1996 3291 3791 2000 2022 10395 1999 1996 2279 2048 2000 2093 3134 1012 1055 1012 2848 2773 2368 1010 2472 1997 9274 1005 1055 19900 2470 2415 1999 2662 1010 2409 12060 2002 1005 1055 9657 2673 2097 2022 2551 7919 1999 1996 2279 2261 2420 1012 1996 12076 2001 2764 2012 19900 1012 14804 4402 1005 1055 4668 7787 2020 2357 2006 2000 16865 1998 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9274 1005 1055 14751 20478 10566 7596 2098 2046 2686 2397 5958 2013 3448 1010 28190 3712 3422 2545 2247 1996 2264 3023 1012 2021 1996 14804 4402 12076 2855 2743 2046 3941 4390 1010 1998 2096 9274 8916 3071 2220 5095 2008 1996 11926 15113 2001 3647 1998 2006 1037 3819 2650 2005 1996 4231 1010 4584 8969 1996 3291 3791 2000 2022 10395 1999 1996 2279 2048 2000 2093 3134 1012 1055 1012 2848 2773 2368 1010 2472 1997 9274 1005 1055 19900 2470 2415 1999 2662 1010 2409 12060 2002 1005 1055 9657 2673 2097 2022 2551 7919 1999 1996 2279 2261 2420 1012 1996 12076 2001 2764 2012 19900 1012 14804 4402 1005 1055 4668 7787 2020 2357 2006 2000 16865 1998 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — police in ferguson , missouri , used u . s . military equipment during protests this summer . they sent officers with armored vehicles , assault rifles and body armor to at times peaceful marches . now , president barack obama is ordering up new rules for giving local police agencies such weapons . obama also proposed to spend $ 263 million over three years to expand training . it will increase the use of body cameras for recording police interaction with the public . the proposal includes $ 75 million that would help buy as many as 50 , 000 cameras . cameras like these might have provided more information in michael brown ' s death . in august , the unarmed black [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — police in ferguson , missouri , used u . s . military equipment during protests this summer . they sent officers with armored vehicles , assault rifles and body armor to at times peaceful marches . now , president barack obama is ordering up new rules for giving local police agencies such weapons . obama also proposed to spend $ 263 million over three years to expand training . it will increase the use of body cameras for recording police interaction with the public . the proposal includes $ 75 million that would help buy as many as 50 , 000 cameras . cameras like these might have provided more information in michael brown ' s death . in august , the unarmed black [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2610 1999 11262 1010 5284 1010 2109 1057 1012 1055 1012 2510 3941 2076 8090 2023 2621 1012 2027 2741 3738 2007 10612 4683 1010 6101 9494 1998 2303 8177 2000 2012 2335 9379 20691 1012 2085 1010 2343 13857 8112 2003 13063 2039 2047 3513 2005 3228 2334 2610 6736 2107 4255 1012 8112 2036 3818 2000 5247 1002 25246 2454 2058 2093 2086 2000 7818 2731 1012 2009 2097 3623 1996 2224 1997 2303 8629 2005 3405 2610 8290 2007 1996 2270 1012 1996 6378 2950 1002 4293 2454 2008 2052 2393 4965 2004 2116 2004 2753 1010 2199 8629 1012 8629 2066 2122 2453 2031 3024 2062 2592 1999 2745 2829 1005 1055 2331 1012 1999 2257 1010 1996 23206 2304 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2610 1999 11262 1010 5284 1010 2109 1057 1012 1055 1012 2510 3941 2076 8090 2023 2621 1012 2027 2741 3738 2007 10612 4683 1010 6101 9494 1998 2303 8177 2000 2012 2335 9379 20691 1012 2085 1010 2343 13857 8112 2003 13063 2039 2047 3513 2005 3228 2334 2610 6736 2107 4255 1012 8112 2036 3818 2000 5247 1002 25246 2454 2058 2093 2086 2000 7818 2731 1012 2009 2097 3623 1996 2224 1997 2303 8629 2005 3405 2610 8290 2007 1996 2270 1012 1996 6378 2950 1002 4293 2454 2008 2052 2393 4965 2004 2116 2004 2753 1010 2199 8629 1012 8629 2066 2122 2453 2031 3024 2062 2592 1999 2745 2829 1005 1055 2331 1012 1999 2257 1010 1996 23206 2304 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a series of recent discoveries has scientists rev ##ising their image of mars : they are now beginning to suspect that the red planet ' s geology may be more complex and earth - like than previously imagined . new data suggests that the planet may have small bits of water spread around its surface . and it now seems that the planet ' s interior could once have been more geological ##ly mature than previously thought . the new data is courtesy of nasa ' s curiosity , a car - sized robotic rover that has been exploring parts of the surface of mars . launched in 2011 , the highly sophisticated vehicle is armed with an impressive arsenal of scientific instruments . in 2012 [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a series of recent discoveries has scientists rev ##ising their image of mars : they are now beginning to suspect that the red planet ' s geology may be more complex and earth - like than previously imagined . new data suggests that the planet may have small bits of water spread around its surface . and it now seems that the planet ' s interior could once have been more geological ##ly mature than previously thought . the new data is courtesy of nasa ' s curiosity , a car - sized robotic rover that has been exploring parts of the surface of mars . launched in 2011 , the highly sophisticated vehicle is armed with an impressive arsenal of scientific instruments . in 2012 [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2186 1997 3522 15636 2038 6529 7065 9355 2037 3746 1997 7733 1024 2027 2024 2085 2927 2000 8343 2008 1996 2417 4774 1005 1055 13404 2089 2022 2062 3375 1998 3011 1011 2066 2084 3130 8078 1012 2047 2951 6083 2008 1996 4774 2089 2031 2235 9017 1997 2300 3659 2105 2049 3302 1012 1998 2009 2085 3849 2008 1996 4774 1005 1055 4592 2071 2320 2031 2042 2062 9843 2135 9677 2084 3130 2245 1012 1996 2047 2951 2003 14571 1997 9274 1005 1055 10628 1010 1037 2482 1011 7451 20478 13631 2008 2038 2042 11131 3033 1997 1996 3302 1997 7733 1012 3390 1999 2249 1010 1996 3811 12138 4316 2003 4273 2007 2019 8052 9433 1997 4045 5693 1012 1999 2262 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2186 1997 3522 15636 2038 6529 7065 9355 2037 3746 1997 7733 1024 2027 2024 2085 2927 2000 8343 2008 1996 2417 4774 1005 1055 13404 2089 2022 2062 3375 1998 3011 1011 2066 2084 3130 8078 1012 2047 2951 6083 2008 1996 4774 2089 2031 2235 9017 1997 2300 3659 2105 2049 3302 1012 1998 2009 2085 3849 2008 1996 4774 1005 1055 4592 2071 2320 2031 2042 2062 9843 2135 9677 2084 3130 2245 1012 1996 2047 2951 2003 14571 1997 9274 1005 1055 10628 1010 1037 2482 1011 7451 20478 13631 2008 2038 2042 11131 3033 1997 1996 3302 1997 7733 1012 3390 1999 2249 1010 1996 3811 12138 4316 2003 4273 2007 2019 8052 9433 1997 4045 5693 1012 1999 2262 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for our data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        \"\"\"\n",
        "        Removed since they are only \n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        \"\"\"\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            # \"f1_score\": f1_score,\n",
        "            # \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### These hyperparameters are recommended by the authors for fine-tuning BERT ###\n",
        "\n",
        "# Compute train and warmup steps from batch size\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 3e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "outputId": "3bdec77a-e6fa-4b14-eadb-69bf0b457d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "print('Number of training steps: %d' % num_train_steps)\n",
        "print('Number of warmup steps: %d' % num_warmup_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training steps: 803\n",
            "Number of warmup steps: 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "71eac03d-05d1-47bc-8c28-74313d3c1ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/My Drive/585/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb5afea5908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/My Drive/585/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb5afea5908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Finish setting up. Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "a17cf339-8704-4ee7-8f37-be928fdbd457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.6769224, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.6769224, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.56523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.56523\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6915046, step = 100 (63.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6915046, step = 100 (63.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10008\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.948285, step = 200 (47.612 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.948285, step = 200 (47.612 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09813\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6294203, step = 300 (47.662 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6294203, step = 300 (47.662 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09994\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.69733214, step = 400 (47.620 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.69733214, step = 400 (47.620 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.83309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.83309\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.24030101, step = 500 (54.557 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.24030101, step = 500 (54.557 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10164\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.38211882, step = 600 (47.578 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.38211882, step = 600 (47.578 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.09908\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18672721, step = 700 (47.644 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18672721, step = 700 (47.644 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 2.10182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.08411118, step = 800 (47.574 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.08411118, step = 800 (47.574 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 803 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 803 into /content/gdrive/My Drive/585/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.19443211.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.19443211.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:07:59.604802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Evaluation with our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "outputId": "b1a08e14-9d0c-42ce-f7b8-33a3cebdc138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-12-09T05:55:29Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-12-09T05:55:29Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-12-09-05:55:43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-12-09-05:55:43\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 803: eval_accuracy = 0.6904512, false_negatives = 32.0, false_positives = 60.0, global_step = 803, loss = 0.8381214, precision = 0.9240506, recall = 0.95800525, true_negatives = 131.0, true_positives = 730.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 803: eval_accuracy = 0.6904512, false_negatives = 32.0, false_positives = 60.0, global_step = 803, loss = 0.8381214, precision = 0.9240506, recall = 0.95800525, true_negatives = 131.0, true_positives = 730.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 803: /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 803: /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.6904512,\n",
              " 'false_negatives': 32.0,\n",
              " 'false_positives': 60.0,\n",
              " 'global_step': 803,\n",
              " 'loss': 0.8381214,\n",
              " 'precision': 0.9240506,\n",
              " 'recall': 0.95800525,\n",
              " 'true_negatives': 131.0,\n",
              " 'true_positives': 730.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Now let's examine some of the examples that we got wrong:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQJTztEUCmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wrong_examples(test_set):\n",
        "  input_examples = test_set.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  \n",
        "  pred_labels = [pred['labels'] for pred in list(predictions)]\n",
        "\n",
        "  wrong_out = [] \n",
        "  for i in range(len(test_set)):\n",
        "    pred_label = pred_labels[i]\n",
        "    gt_label = test_set.iloc[i].label\n",
        "    if pred_label != gt_label:\n",
        "      wrong_out.append([gt_label, pred_label, test_set.iloc[i].text])\n",
        "  \n",
        "  return wrong_out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BflJRmiiUMZS",
        "colab_type": "code",
        "outputId": "66b6b4d1-f863-467b-8290-a5d461d9b962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "wrong_examples = get_wrong_examples(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/585/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va3-9IuDVLXH",
        "colab_type": "code",
        "outputId": "72d03b19-050e-4460-84db-d9bec8cdaadb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "more_than_two_lvl = 0\n",
        "for gt_label, pred_label, text in wrong_examples:\n",
        "  if abs(gt_label > pred_label) > 1:\n",
        "    more_than_two_lvl += 1\n",
        "  print('Ground-truth label: %d. Predicted label: %d. Text:' % (gt_label, pred_label))\n",
        "  # print(text)\n",
        "  print()\n",
        "\n",
        "print('BERT got %d examples wrong in total, in which %d were off by at least 2 levels.' % (len(wrong_examples), more_than_two_lvl))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 4. Predicted label: 3. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 2. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 0. Predicted label: 1. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 0. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 3. Predicted label: 4. Text:\n",
            "\n",
            "Ground-truth label: 1. Predicted label: 2. Text:\n",
            "\n",
            "BERT got 290 examples wrong in total, in which 0 were off by at least 2 levels.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7JEjZW9aqzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}