{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTRank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOkgtquc2ziK",
        "colab_type": "text"
      },
      "source": [
        "Google Colab link: https://colab.research.google.com/drive/1PLyNxB430viZId2-pEFFNWUkYfYsv2s9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvrNN4u07int",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modifications copyright (C) 2013 Hieu Phan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "#Document Classification by Reading Difficulty with BERT #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "source": [
        "Import the neccessary libraries.  \n",
        "Note: the BERT library below only works with Tensorflow version under 2.0.0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "66178be5-f46c-4eeb-c130-0ae0b4cacee5"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package from Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "33162aec-8e86-4928-ba11-3e320af4b69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "outputId": "5e7e91a4-a3e2-4d28-fede-454dfa259f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "source": [
        "Mount the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "ed493985-08ef-499b-d37c-bb89cf4df228",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JriF_RpT96o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "source": [
        "Load the Newsela dataset from the mounted drive. These two datasets are generated from the Newsela corpus using stratified sampling with training/test ratio of 9:1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# Load both training and test set from Google Drive into DataFrames.\n",
        "def load_dataset(path='/content/gdrive/My Drive/585/'):\n",
        "  training = pd.read_csv(path + 'training_newsela.csv')\n",
        "  test = pd.read_csv(path + 'test_newsela.csv')\n",
        "  return training, test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abfwdn-g135",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "source": [
        "Print out the sizes of our training and test sets and also an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "outputId": "b434a76d-e088-4ff7-ec8e-587f496b5340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Training set contains %d examples' % len(train))\n",
        "print('Test set contains %d examples' % len(test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set contains 8570 examples\n",
            "Test set contains 953 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWIz-bzm_jiB",
        "colab_type": "code",
        "outputId": "4989e644-19d0-43b3-ddee-8d7e3541385a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "idx = np.random.randint(0, len(train))\n",
        "print('Example from the training set:')\n",
        "print('Text:' + train.iloc[idx].text)\n",
        "print('Difficulty (between 0 and 4): %d' % train.iloc[idx].label) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example from the training set:\n",
            "Text:DELEON SPRINGS, Fla. — The cow looked around David Strawn's pickup truck. She was hoping to find something good to eat.\n",
            "\n",
            "Strawn is a rancher. He is 79 years old and lives in the state of Florida. For many years, his family raised cows, lambs and pigs. Then Strawn got rid of the other animals and started raising grass-fed cattle.\n",
            "\n",
            "## No Cages For These Cows\n",
            "\n",
            "Strawn likes to eat meat. He wants to make sure the animals he eats live a good life. He thinks grass-fed cattle live good lives. Grass-fed cattle eat grass, not grains. They do not live in cages. They are not forced to grow fatter.\n",
            "\n",
            "\"These guys enjoy our pastures, the lakes,\" said Strawn. \"They get gentle treatment.\"\n",
            "\n",
            "Supporters of grass-fed meat like it for several reasons. They say it has less fat and is healthier than regular meat. They also say the animals are treated better.\n",
            "\n",
            "Not everyone agrees. Some people think grass-fed meat is no better than regular meat.\n",
            "\n",
            "## This Meat Costs More\n",
            "\n",
            "One thing is certain. Grass-fed meat usually costs more money.\n",
            "\n",
            "Erika Maier is a schoolteacher who has been buying grass-fed meat for several years. She buys it from David Strawn's ranch. This month she and several other families bought a cow that weighed 935 pounds. It cost more than $2,000. The families split the cost.\n",
            "\n",
            "Maier freezes her meat. She makes dog food out of the parts she will not eat. She gives it to her German shepherd, Buddy.\n",
            "\n",
            "## He Cares For His Ranch\n",
            "\n",
            "Strawn used to sell his meat to restaurants in South Florida. Then he decided to sell to people directly. He wanted to get to know them better. Strawn loves his work. He likes to take care of the trees and wildlife that share the land with his cattle.\n",
            "\n",
            "\"I really like growing things,\" said Strawn, who grew up on the ranch. \"I like caring for things.\"\n",
            "Difficulty (between 0 and 4): 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab_type": "code",
        "outputId": "7fe770b7-6078-4176-8bdc-15002a628416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'text', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "label_list = [0, 1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "e2b4f8a0-0236-4f32-800d-5f811c8643e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "source": [
        "Tokenized example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "5666cb0c-0ed6-440f-c7e1-fee17852854e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Convert to features for BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "9988fc69-7c72-4e1e-abe0-d3c62dba1f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We'll set sequences to be at most 256 tokens long.\n",
        "MAX_SEQ_LENGTH = 256\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 8570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 8570\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lo ##me , togo — soccer officials in the african country of togo are afraid to let their team travel to guinea for a match . guinea is where the outbreak of the deadly e ##bola disease started . to protect their players , togo is asking that the match be moved to another country . the soccer match is part of the african cup . it is the biggest soccer tournament in africa and it happens every year . soccer officials fear the spread of the painful and con ##tag ##ious disease could ruin the african cup ' s final qualifying round . the winners of the qualifying round will get to play in the finals in morocco . e ##bola has killed nearly 1 , 000 people in west africa . more than 300 people are believed to have died from the virus in the african country of guinea . after spreading from guinea , it traveled to sierra leone and liberia . most recently , people have been getting sick in nigeria . # # togo not end ##anger ##ing its players games involving sierra leone are already being looked at . the country said it would not host any soccer matches for now because of e ##bola . togo ' s request to move the match would affect the final group stage in guinea . it is set to take place the first week of september . togo ' s soccer officials said that its players feared traveling to guinea [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lo ##me , togo — soccer officials in the african country of togo are afraid to let their team travel to guinea for a match . guinea is where the outbreak of the deadly e ##bola disease started . to protect their players , togo is asking that the match be moved to another country . the soccer match is part of the african cup . it is the biggest soccer tournament in africa and it happens every year . soccer officials fear the spread of the painful and con ##tag ##ious disease could ruin the african cup ' s final qualifying round . the winners of the qualifying round will get to play in the finals in morocco . e ##bola has killed nearly 1 , 000 people in west africa . more than 300 people are believed to have died from the virus in the african country of guinea . after spreading from guinea , it traveled to sierra leone and liberia . most recently , people have been getting sick in nigeria . # # togo not end ##anger ##ing its players games involving sierra leone are already being looked at . the country said it would not host any soccer matches for now because of e ##bola . togo ' s request to move the match would affect the final group stage in guinea . it is set to take place the first week of september . togo ' s soccer officials said that its players feared traveling to guinea [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8840 4168 1010 23588 1517 4715 4584 1999 1996 3060 2406 1997 23588 2024 4452 2000 2292 2037 2136 3604 2000 7102 2005 1037 2674 1012 7102 2003 2073 1996 8293 1997 1996 9252 1041 24290 4295 2318 1012 2000 4047 2037 2867 1010 23588 2003 4851 2008 1996 2674 2022 2333 2000 2178 2406 1012 1996 4715 2674 2003 2112 1997 1996 3060 2452 1012 2009 2003 1996 5221 4715 2977 1999 3088 1998 2009 6433 2296 2095 1012 4715 4584 3571 1996 3659 1997 1996 9145 1998 9530 15900 6313 4295 2071 10083 1996 3060 2452 1005 1055 2345 6042 2461 1012 1996 4791 1997 1996 6042 2461 2097 2131 2000 2377 1999 1996 4399 1999 9835 1012 1041 24290 2038 2730 3053 1015 1010 2199 2111 1999 2225 3088 1012 2062 2084 3998 2111 2024 3373 2000 2031 2351 2013 1996 7865 1999 1996 3060 2406 1997 7102 1012 2044 9359 2013 7102 1010 2009 6158 2000 7838 13363 1998 18039 1012 2087 3728 1010 2111 2031 2042 2893 5305 1999 7387 1012 1001 1001 23588 2025 2203 25121 2075 2049 2867 2399 5994 7838 13363 2024 2525 2108 2246 2012 1012 1996 2406 2056 2009 2052 2025 3677 2151 4715 3503 2005 2085 2138 1997 1041 24290 1012 23588 1005 1055 5227 2000 2693 1996 2674 2052 7461 1996 2345 2177 2754 1999 7102 1012 2009 2003 2275 2000 2202 2173 1996 2034 2733 1997 2244 1012 23588 1005 1055 4715 4584 2056 2008 2049 2867 8615 7118 2000 7102 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 8840 4168 1010 23588 1517 4715 4584 1999 1996 3060 2406 1997 23588 2024 4452 2000 2292 2037 2136 3604 2000 7102 2005 1037 2674 1012 7102 2003 2073 1996 8293 1997 1996 9252 1041 24290 4295 2318 1012 2000 4047 2037 2867 1010 23588 2003 4851 2008 1996 2674 2022 2333 2000 2178 2406 1012 1996 4715 2674 2003 2112 1997 1996 3060 2452 1012 2009 2003 1996 5221 4715 2977 1999 3088 1998 2009 6433 2296 2095 1012 4715 4584 3571 1996 3659 1997 1996 9145 1998 9530 15900 6313 4295 2071 10083 1996 3060 2452 1005 1055 2345 6042 2461 1012 1996 4791 1997 1996 6042 2461 2097 2131 2000 2377 1999 1996 4399 1999 9835 1012 1041 24290 2038 2730 3053 1015 1010 2199 2111 1999 2225 3088 1012 2062 2084 3998 2111 2024 3373 2000 2031 2351 2013 1996 7865 1999 1996 3060 2406 1997 7102 1012 2044 9359 2013 7102 1010 2009 6158 2000 7838 13363 1998 18039 1012 2087 3728 1010 2111 2031 2042 2893 5305 1999 7387 1012 1001 1001 23588 2025 2203 25121 2075 2049 2867 2399 5994 7838 13363 2024 2525 2108 2246 2012 1012 1996 2406 2056 2009 2052 2025 3677 2151 4715 3503 2005 2085 2138 1997 1041 24290 1012 23588 1005 1055 5227 2000 2693 1996 2674 2052 7461 1996 2345 2177 2754 1999 7102 1012 2009 2003 2275 2000 2202 2173 1996 2034 2733 1997 2244 1012 23588 1005 1055 4715 4584 2056 2008 2049 2867 8615 7118 2000 7102 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] los angeles — the sar ##dine fishing boat eileen glided through moon ##lit waters as it traveled from san pedro to santa catalina island off the california coast . its tired - eyed captain had grown more desperate as the evening wore on . but after 12 hours and $ 1 , 000 worth of fuel , corbin hanson and his crew returned to port . the hadn ' t caught a single small , silvery sar ##dine . \" tonight ' s pretty reflective of how things have been going , \" hanson said . \" not very well . \" to blame is the biggest sar ##dine crash in generations , which has made schools of the fish rare on the west coast . the decline has caused steep cuts in the amount of fish fishermen are allowed to catch . scientists say the effects are probably radiating throughout the ecosystem . brown pe ##lica ##ns , sea lions and other predators that rely on the oil ##y , energy - rich fish for food could be starving . if sar ##dine ##s don ' t recover soon , experts warn , there could be serious problems . the west coast ' s marine animals , sea ##birds and fishermen could suffer for years . # # climate cycle one cu ##lp ##rit sar ##dine populations are famously unpredictable . but for reasons that are unclear , the decline is the steep ##est since the mid - 20th century . and their [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] los angeles — the sar ##dine fishing boat eileen glided through moon ##lit waters as it traveled from san pedro to santa catalina island off the california coast . its tired - eyed captain had grown more desperate as the evening wore on . but after 12 hours and $ 1 , 000 worth of fuel , corbin hanson and his crew returned to port . the hadn ' t caught a single small , silvery sar ##dine . \" tonight ' s pretty reflective of how things have been going , \" hanson said . \" not very well . \" to blame is the biggest sar ##dine crash in generations , which has made schools of the fish rare on the west coast . the decline has caused steep cuts in the amount of fish fishermen are allowed to catch . scientists say the effects are probably radiating throughout the ecosystem . brown pe ##lica ##ns , sea lions and other predators that rely on the oil ##y , energy - rich fish for food could be starving . if sar ##dine ##s don ' t recover soon , experts warn , there could be serious problems . the west coast ' s marine animals , sea ##birds and fishermen could suffer for years . # # climate cycle one cu ##lp ##rit sar ##dine populations are famously unpredictable . but for reasons that are unclear , the decline is the steep ##est since the mid - 20th century . and their [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3050 3349 1517 1996 18906 10672 5645 4049 20495 26936 2083 4231 15909 5380 2004 2009 6158 2013 2624 7707 2000 4203 22326 2479 2125 1996 2662 3023 1012 2049 5458 1011 7168 2952 2018 4961 2062 7143 2004 1996 3944 5078 2006 1012 2021 2044 2260 2847 1998 1002 1015 1010 2199 4276 1997 4762 1010 24003 17179 1998 2010 3626 2513 2000 3417 1012 1996 2910 1005 1056 3236 1037 2309 2235 1010 21666 18906 10672 1012 1000 3892 1005 1055 3492 21346 1997 2129 2477 2031 2042 2183 1010 1000 17179 2056 1012 1000 2025 2200 2092 1012 1000 2000 7499 2003 1996 5221 18906 10672 5823 1999 8213 1010 2029 2038 2081 2816 1997 1996 3869 4678 2006 1996 2225 3023 1012 1996 6689 2038 3303 9561 7659 1999 1996 3815 1997 3869 16532 2024 3039 2000 4608 1012 6529 2360 1996 3896 2024 2763 23229 2802 1996 16927 1012 2829 21877 19341 3619 1010 2712 7212 1998 2060 12630 2008 11160 2006 1996 3514 2100 1010 2943 1011 4138 3869 2005 2833 2071 2022 18025 1012 2065 18906 10672 2015 2123 1005 1056 8980 2574 1010 8519 11582 1010 2045 2071 2022 3809 3471 1012 1996 2225 3023 1005 1055 3884 4176 1010 2712 12887 1998 16532 2071 9015 2005 2086 1012 1001 1001 4785 5402 2028 12731 14277 14778 18906 10672 7080 2024 18172 21446 1012 2021 2005 4436 2008 2024 10599 1010 1996 6689 2003 1996 9561 4355 2144 1996 3054 1011 3983 2301 1012 1998 2037 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3050 3349 1517 1996 18906 10672 5645 4049 20495 26936 2083 4231 15909 5380 2004 2009 6158 2013 2624 7707 2000 4203 22326 2479 2125 1996 2662 3023 1012 2049 5458 1011 7168 2952 2018 4961 2062 7143 2004 1996 3944 5078 2006 1012 2021 2044 2260 2847 1998 1002 1015 1010 2199 4276 1997 4762 1010 24003 17179 1998 2010 3626 2513 2000 3417 1012 1996 2910 1005 1056 3236 1037 2309 2235 1010 21666 18906 10672 1012 1000 3892 1005 1055 3492 21346 1997 2129 2477 2031 2042 2183 1010 1000 17179 2056 1012 1000 2025 2200 2092 1012 1000 2000 7499 2003 1996 5221 18906 10672 5823 1999 8213 1010 2029 2038 2081 2816 1997 1996 3869 4678 2006 1996 2225 3023 1012 1996 6689 2038 3303 9561 7659 1999 1996 3815 1997 3869 16532 2024 3039 2000 4608 1012 6529 2360 1996 3896 2024 2763 23229 2802 1996 16927 1012 2829 21877 19341 3619 1010 2712 7212 1998 2060 12630 2008 11160 2006 1996 3514 2100 1010 2943 1011 4138 3869 2005 2833 2071 2022 18025 1012 2065 18906 10672 2015 2123 1005 1056 8980 2574 1010 8519 11582 1010 2045 2071 2022 3809 3471 1012 1996 2225 3023 1005 1055 3884 4176 1010 2712 12887 1998 16532 2071 9015 2005 2086 1012 1001 1001 4785 5402 2028 12731 14277 14778 18906 10672 7080 2024 18172 21446 1012 2021 2005 4436 2008 2024 10599 1010 1996 6689 2003 1996 9561 4355 2144 1996 3054 1011 3983 2301 1012 1998 2037 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] nasa ' s newest robotic explorer rocket ##ed into space late friday from virginia , dazzling sky watch ##ers along the east coast . but the lad ##ee spacecraft quickly ran into equipment trouble , and while nasa assured everyone early saturday that the lunar probe was safe and on a perfect track for the moon , officials acknowledged the problem needs to be resolved in the next two to three weeks . s . peter word ##en , director of nasa ' s ames research center in california , told reporters he ' s confident everything will be working properly in the next few days . the spacecraft was developed at ames . lad ##ee ' s reaction wheels were turned on to orient and stabilize the spacecraft , which was spinning too fast after it separated from the final rocket stage , word ##en said . but the computer automatically shut the wheels down , apparently because of excess current . he speculated the wheels may have been running a little fast . # # \" gods ##peed on your journey \" word ##en stressed there is no rush to \" get these bugs iron ##ed out . \" the lad ##ee spacecraft is charged with studying the lunar atmosphere and dust . it soared aboard an unmanned min ##ota ##ur rocket a little before midnight from virginia ' s eastern shore . \" gods ##peed on your journey to the moon , lad ##ee , \" launch control said . flight [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] nasa ' s newest robotic explorer rocket ##ed into space late friday from virginia , dazzling sky watch ##ers along the east coast . but the lad ##ee spacecraft quickly ran into equipment trouble , and while nasa assured everyone early saturday that the lunar probe was safe and on a perfect track for the moon , officials acknowledged the problem needs to be resolved in the next two to three weeks . s . peter word ##en , director of nasa ' s ames research center in california , told reporters he ' s confident everything will be working properly in the next few days . the spacecraft was developed at ames . lad ##ee ' s reaction wheels were turned on to orient and stabilize the spacecraft , which was spinning too fast after it separated from the final rocket stage , word ##en said . but the computer automatically shut the wheels down , apparently because of excess current . he speculated the wheels may have been running a little fast . # # \" gods ##peed on your journey \" word ##en stressed there is no rush to \" get these bugs iron ##ed out . \" the lad ##ee spacecraft is charged with studying the lunar atmosphere and dust . it soared aboard an unmanned min ##ota ##ur rocket a little before midnight from virginia ' s eastern shore . \" gods ##peed on your journey to the moon , lad ##ee , \" launch control said . flight [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9274 1005 1055 14751 20478 10566 7596 2098 2046 2686 2397 5958 2013 3448 1010 28190 3712 3422 2545 2247 1996 2264 3023 1012 2021 1996 14804 4402 12076 2855 2743 2046 3941 4390 1010 1998 2096 9274 8916 3071 2220 5095 2008 1996 11926 15113 2001 3647 1998 2006 1037 3819 2650 2005 1996 4231 1010 4584 8969 1996 3291 3791 2000 2022 10395 1999 1996 2279 2048 2000 2093 3134 1012 1055 1012 2848 2773 2368 1010 2472 1997 9274 1005 1055 19900 2470 2415 1999 2662 1010 2409 12060 2002 1005 1055 9657 2673 2097 2022 2551 7919 1999 1996 2279 2261 2420 1012 1996 12076 2001 2764 2012 19900 1012 14804 4402 1005 1055 4668 7787 2020 2357 2006 2000 16865 1998 27790 1996 12076 1010 2029 2001 9419 2205 3435 2044 2009 5459 2013 1996 2345 7596 2754 1010 2773 2368 2056 1012 2021 1996 3274 8073 3844 1996 7787 2091 1010 4593 2138 1997 9987 2783 1012 2002 15520 1996 7787 2089 2031 2042 2770 1037 2210 3435 1012 1001 1001 1000 5932 25599 2006 2115 4990 1000 2773 2368 13233 2045 2003 2053 5481 2000 1000 2131 2122 12883 3707 2098 2041 1012 1000 1996 14804 4402 12076 2003 5338 2007 5702 1996 11926 7224 1998 6497 1012 2009 29127 7548 2019 24075 8117 17287 3126 7596 1037 2210 2077 7090 2013 3448 1005 1055 2789 5370 1012 1000 5932 25599 2006 2115 4990 2000 1996 4231 1010 14804 4402 1010 1000 4888 2491 2056 1012 3462 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9274 1005 1055 14751 20478 10566 7596 2098 2046 2686 2397 5958 2013 3448 1010 28190 3712 3422 2545 2247 1996 2264 3023 1012 2021 1996 14804 4402 12076 2855 2743 2046 3941 4390 1010 1998 2096 9274 8916 3071 2220 5095 2008 1996 11926 15113 2001 3647 1998 2006 1037 3819 2650 2005 1996 4231 1010 4584 8969 1996 3291 3791 2000 2022 10395 1999 1996 2279 2048 2000 2093 3134 1012 1055 1012 2848 2773 2368 1010 2472 1997 9274 1005 1055 19900 2470 2415 1999 2662 1010 2409 12060 2002 1005 1055 9657 2673 2097 2022 2551 7919 1999 1996 2279 2261 2420 1012 1996 12076 2001 2764 2012 19900 1012 14804 4402 1005 1055 4668 7787 2020 2357 2006 2000 16865 1998 27790 1996 12076 1010 2029 2001 9419 2205 3435 2044 2009 5459 2013 1996 2345 7596 2754 1010 2773 2368 2056 1012 2021 1996 3274 8073 3844 1996 7787 2091 1010 4593 2138 1997 9987 2783 1012 2002 15520 1996 7787 2089 2031 2042 2770 1037 2210 3435 1012 1001 1001 1000 5932 25599 2006 2115 4990 1000 2773 2368 13233 2045 2003 2053 5481 2000 1000 2131 2122 12883 3707 2098 2041 1012 1000 1996 14804 4402 12076 2003 5338 2007 5702 1996 11926 7224 1998 6497 1012 2009 29127 7548 2019 24075 8117 17287 3126 7596 1037 2210 2077 7090 2013 3448 1005 1055 2789 5370 1012 1000 5932 25599 2006 2115 4990 2000 1996 4231 1010 14804 4402 1010 1000 4888 2491 2056 1012 3462 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — police in ferguson , missouri , used u . s . military equipment during protests this summer . they sent officers with armored vehicles , assault rifles and body armor to at times peaceful marches . now , president barack obama is ordering up new rules for giving local police agencies such weapons . obama also proposed to spend $ 263 million over three years to expand training . it will increase the use of body cameras for recording police interaction with the public . the proposal includes $ 75 million that would help buy as many as 50 , 000 cameras . cameras like these might have provided more information in michael brown ' s death . in august , the unarmed black 18 - year - old was shot by a white police officer in ferguson . the president ' s new rules came along with the release of a new white house review . it criticized the \" surplus \" programs of the department of defense and other federal agencies . the surplus programs give military supplies that are no longer being used by the military to local police departments . # # new rules follow protests the government found that the program was run by a mixed - up set of rules and practices . for example , there is no clear sign that all police are correctly trained and ready to use the military equipment they receive . the results of the review came a week after [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — police in ferguson , missouri , used u . s . military equipment during protests this summer . they sent officers with armored vehicles , assault rifles and body armor to at times peaceful marches . now , president barack obama is ordering up new rules for giving local police agencies such weapons . obama also proposed to spend $ 263 million over three years to expand training . it will increase the use of body cameras for recording police interaction with the public . the proposal includes $ 75 million that would help buy as many as 50 , 000 cameras . cameras like these might have provided more information in michael brown ' s death . in august , the unarmed black 18 - year - old was shot by a white police officer in ferguson . the president ' s new rules came along with the release of a new white house review . it criticized the \" surplus \" programs of the department of defense and other federal agencies . the surplus programs give military supplies that are no longer being used by the military to local police departments . # # new rules follow protests the government found that the program was run by a mixed - up set of rules and practices . for example , there is no clear sign that all police are correctly trained and ready to use the military equipment they receive . the results of the review came a week after [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2610 1999 11262 1010 5284 1010 2109 1057 1012 1055 1012 2510 3941 2076 8090 2023 2621 1012 2027 2741 3738 2007 10612 4683 1010 6101 9494 1998 2303 8177 2000 2012 2335 9379 20691 1012 2085 1010 2343 13857 8112 2003 13063 2039 2047 3513 2005 3228 2334 2610 6736 2107 4255 1012 8112 2036 3818 2000 5247 1002 25246 2454 2058 2093 2086 2000 7818 2731 1012 2009 2097 3623 1996 2224 1997 2303 8629 2005 3405 2610 8290 2007 1996 2270 1012 1996 6378 2950 1002 4293 2454 2008 2052 2393 4965 2004 2116 2004 2753 1010 2199 8629 1012 8629 2066 2122 2453 2031 3024 2062 2592 1999 2745 2829 1005 1055 2331 1012 1999 2257 1010 1996 23206 2304 2324 1011 2095 1011 2214 2001 2915 2011 1037 2317 2610 2961 1999 11262 1012 1996 2343 1005 1055 2047 3513 2234 2247 2007 1996 2713 1997 1037 2047 2317 2160 3319 1012 2009 6367 1996 1000 15726 1000 3454 1997 1996 2533 1997 3639 1998 2060 2976 6736 1012 1996 15726 3454 2507 2510 6067 2008 2024 2053 2936 2108 2109 2011 1996 2510 2000 2334 2610 7640 1012 1001 1001 2047 3513 3582 8090 1996 2231 2179 2008 1996 2565 2001 2448 2011 1037 3816 1011 2039 2275 1997 3513 1998 6078 1012 2005 2742 1010 2045 2003 2053 3154 3696 2008 2035 2610 2024 11178 4738 1998 3201 2000 2224 1996 2510 3941 2027 4374 1012 1996 3463 1997 1996 3319 2234 1037 2733 2044 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2610 1999 11262 1010 5284 1010 2109 1057 1012 1055 1012 2510 3941 2076 8090 2023 2621 1012 2027 2741 3738 2007 10612 4683 1010 6101 9494 1998 2303 8177 2000 2012 2335 9379 20691 1012 2085 1010 2343 13857 8112 2003 13063 2039 2047 3513 2005 3228 2334 2610 6736 2107 4255 1012 8112 2036 3818 2000 5247 1002 25246 2454 2058 2093 2086 2000 7818 2731 1012 2009 2097 3623 1996 2224 1997 2303 8629 2005 3405 2610 8290 2007 1996 2270 1012 1996 6378 2950 1002 4293 2454 2008 2052 2393 4965 2004 2116 2004 2753 1010 2199 8629 1012 8629 2066 2122 2453 2031 3024 2062 2592 1999 2745 2829 1005 1055 2331 1012 1999 2257 1010 1996 23206 2304 2324 1011 2095 1011 2214 2001 2915 2011 1037 2317 2610 2961 1999 11262 1012 1996 2343 1005 1055 2047 3513 2234 2247 2007 1996 2713 1997 1037 2047 2317 2160 3319 1012 2009 6367 1996 1000 15726 1000 3454 1997 1996 2533 1997 3639 1998 2060 2976 6736 1012 1996 15726 3454 2507 2510 6067 2008 2024 2053 2936 2108 2109 2011 1996 2510 2000 2334 2610 7640 1012 1001 1001 2047 3513 3582 8090 1996 2231 2179 2008 1996 2565 2001 2448 2011 1037 3816 1011 2039 2275 1997 3513 1998 6078 1012 2005 2742 1010 2045 2003 2053 3154 3696 2008 2035 2610 2024 11178 4738 1998 3201 2000 2224 1996 2510 3941 2027 4374 1012 1996 3463 1997 1996 3319 2234 1037 2733 2044 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a series of recent discoveries has scientists rev ##ising their image of mars : they are now beginning to suspect that the red planet ' s geology may be more complex and earth - like than previously imagined . new data suggests that the planet may have small bits of water spread around its surface . and it now seems that the planet ' s interior could once have been more geological ##ly mature than previously thought . the new data is courtesy of nasa ' s curiosity , a car - sized robotic rover that has been exploring parts of the surface of mars . launched in 2011 , the highly sophisticated vehicle is armed with an impressive arsenal of scientific instruments . in 2012 , it landed in mars ' gale crater . what has scientists excited is that curiosity has detected traces of water . this water is chemical ##ly bound to the martian dust that seems to be covering the entire planet . the finding is among several in the five studies published thursday by the journal science . it may explain mysterious water signals picked up by satellites in orbit around mars . the soil that covers mars ' surface in gale crater seems to consist of two major types , according to data from the rover ' s laser - shooting chemistry and camera ( che ##mc ##am ) instrument . one is a coarse soil with mill ##imeter - wide grains that probably came from the rocks [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a series of recent discoveries has scientists rev ##ising their image of mars : they are now beginning to suspect that the red planet ' s geology may be more complex and earth - like than previously imagined . new data suggests that the planet may have small bits of water spread around its surface . and it now seems that the planet ' s interior could once have been more geological ##ly mature than previously thought . the new data is courtesy of nasa ' s curiosity , a car - sized robotic rover that has been exploring parts of the surface of mars . launched in 2011 , the highly sophisticated vehicle is armed with an impressive arsenal of scientific instruments . in 2012 , it landed in mars ' gale crater . what has scientists excited is that curiosity has detected traces of water . this water is chemical ##ly bound to the martian dust that seems to be covering the entire planet . the finding is among several in the five studies published thursday by the journal science . it may explain mysterious water signals picked up by satellites in orbit around mars . the soil that covers mars ' surface in gale crater seems to consist of two major types , according to data from the rover ' s laser - shooting chemistry and camera ( che ##mc ##am ) instrument . one is a coarse soil with mill ##imeter - wide grains that probably came from the rocks [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2186 1997 3522 15636 2038 6529 7065 9355 2037 3746 1997 7733 1024 2027 2024 2085 2927 2000 8343 2008 1996 2417 4774 1005 1055 13404 2089 2022 2062 3375 1998 3011 1011 2066 2084 3130 8078 1012 2047 2951 6083 2008 1996 4774 2089 2031 2235 9017 1997 2300 3659 2105 2049 3302 1012 1998 2009 2085 3849 2008 1996 4774 1005 1055 4592 2071 2320 2031 2042 2062 9843 2135 9677 2084 3130 2245 1012 1996 2047 2951 2003 14571 1997 9274 1005 1055 10628 1010 1037 2482 1011 7451 20478 13631 2008 2038 2042 11131 3033 1997 1996 3302 1997 7733 1012 3390 1999 2249 1010 1996 3811 12138 4316 2003 4273 2007 2019 8052 9433 1997 4045 5693 1012 1999 2262 1010 2009 5565 1999 7733 1005 14554 11351 1012 2054 2038 6529 7568 2003 2008 10628 2038 11156 10279 1997 2300 1012 2023 2300 2003 5072 2135 5391 2000 1996 20795 6497 2008 3849 2000 2022 5266 1996 2972 4774 1012 1996 4531 2003 2426 2195 1999 1996 2274 2913 2405 9432 2011 1996 3485 2671 1012 2009 2089 4863 8075 2300 7755 3856 2039 2011 14549 1999 8753 2105 7733 1012 1996 5800 2008 4472 7733 1005 3302 1999 14554 11351 3849 2000 8676 1997 2048 2350 4127 1010 2429 2000 2951 2013 1996 13631 1005 1055 9138 1011 5008 6370 1998 4950 1006 18178 12458 3286 1007 6602 1012 2028 2003 1037 20392 5800 2007 4971 19198 1011 2898 17588 2008 2763 2234 2013 1996 5749 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2186 1997 3522 15636 2038 6529 7065 9355 2037 3746 1997 7733 1024 2027 2024 2085 2927 2000 8343 2008 1996 2417 4774 1005 1055 13404 2089 2022 2062 3375 1998 3011 1011 2066 2084 3130 8078 1012 2047 2951 6083 2008 1996 4774 2089 2031 2235 9017 1997 2300 3659 2105 2049 3302 1012 1998 2009 2085 3849 2008 1996 4774 1005 1055 4592 2071 2320 2031 2042 2062 9843 2135 9677 2084 3130 2245 1012 1996 2047 2951 2003 14571 1997 9274 1005 1055 10628 1010 1037 2482 1011 7451 20478 13631 2008 2038 2042 11131 3033 1997 1996 3302 1997 7733 1012 3390 1999 2249 1010 1996 3811 12138 4316 2003 4273 2007 2019 8052 9433 1997 4045 5693 1012 1999 2262 1010 2009 5565 1999 7733 1005 14554 11351 1012 2054 2038 6529 7568 2003 2008 10628 2038 11156 10279 1997 2300 1012 2023 2300 2003 5072 2135 5391 2000 1996 20795 6497 2008 3849 2000 2022 5266 1996 2972 4774 1012 1996 4531 2003 2426 2195 1999 1996 2274 2913 2405 9432 2011 1996 3485 2671 1012 2009 2089 4863 8075 2300 7755 3856 2039 2011 14549 1999 8753 2105 7733 1012 1996 5800 2008 4472 7733 1005 3302 1999 14554 11351 3849 2000 8676 1997 2048 2350 4127 1010 2429 2000 2951 2013 1996 13631 1005 1055 9138 1011 5008 6370 1998 4950 1006 18178 12458 3286 1007 6602 1012 2028 2003 1037 20392 5800 2007 4971 19198 1011 2898 17588 2008 2763 2234 2013 1996 5749 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take the test pass . and that ' s not just bad luck at a single factory or in a single industry . hahn , vice president of organizational development , said that the poor scores on his company ' s math test have been evident for the past six years . he also sits on an aerospace workforce training committee and said that most other washington state suppliers in his industry have been seeing the same problem . \" you could think that even for production , do you really need to know math ? \" said jace ##y wilkins , a spoke ##sw ##oman for the manufacturing institute , an affiliate of the national association of manufacturers . \" but the truth is , you do , [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take the test pass . and that ' s not just bad luck at a single factory or in a single industry . hahn , vice president of organizational development , said that the poor scores on his company ' s math test have been evident for the past six years . he also sits on an aerospace workforce training committee and said that most other washington state suppliers in his industry have been seeing the same problem . \" you could think that even for production , do you really need to know math ? \" said jace ##y wilkins , a spoke ##sw ##oman for the manufacturing institute , an affiliate of the national association of manufacturers . \" but the truth is , you do , [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 1996 3231 3413 1012 1998 2008 1005 1055 2025 2074 2919 6735 2012 1037 2309 4713 2030 1999 1037 2309 3068 1012 24266 1010 3580 2343 1997 13296 2458 1010 2056 2008 1996 3532 7644 2006 2010 2194 1005 1055 8785 3231 2031 2042 10358 2005 1996 2627 2416 2086 1012 2002 2036 7719 2006 2019 13395 14877 2731 2837 1998 2056 2008 2087 2060 2899 2110 20141 1999 2010 3068 2031 2042 3773 1996 2168 3291 1012 1000 2017 2071 2228 2008 2130 2005 2537 1010 2079 2017 2428 2342 2000 2113 8785 1029 1000 2056 10352 2100 22745 1010 1037 3764 26760 20778 2005 1996 5814 2820 1010 2019 8727 1997 1996 2120 2523 1997 8712 1012 1000 2021 1996 3606 2003 1010 2017 2079 1010 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 1996 3231 3413 1012 1998 2008 1005 1055 2025 2074 2919 6735 2012 1037 2309 4713 2030 1999 1037 2309 3068 1012 24266 1010 3580 2343 1997 13296 2458 1010 2056 2008 1996 3532 7644 2006 2010 2194 1005 1055 8785 3231 2031 2042 10358 2005 1996 2627 2416 2086 1012 2002 2036 7719 2006 2019 13395 14877 2731 2837 1998 2056 2008 2087 2060 2899 2110 20141 1999 2010 3068 2031 2042 3773 1996 2168 3291 1012 1000 2017 2071 2228 2008 2130 2005 2537 1010 2079 2017 2428 2342 2000 2113 8785 1029 1000 2056 10352 2100 22745 1010 1037 3764 26760 20778 2005 1996 5814 2820 1010 2019 8727 1997 1996 2120 2523 1997 8712 1012 1000 2021 1996 3606 2003 1010 2017 2079 1010 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . it was attended by about 250 members of the fast - food workforce , labor unions , and religious and legal leaders . corporate spoke ##sm ##en for mcdonald ' s and burger king say wage theft is contrary to company policy and that allegations are investigated . # # when wage theft occurs burger king said thursday that the company \" respects the rights of all workers . \" the company said it \" does not make scheduling , wage or other employment - related decisions for the franchise ##es who independently own and operate almost 100 percent of burger king restaurants . \" these franchise ##es do not work for the burger king company , and instead own their own burger king restaurants . they pay [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . it was attended by about 250 members of the fast - food workforce , labor unions , and religious and legal leaders . corporate spoke ##sm ##en for mcdonald ' s and burger king say wage theft is contrary to company policy and that allegations are investigated . # # when wage theft occurs burger king said thursday that the company \" respects the rights of all workers . \" the company said it \" does not make scheduling , wage or other employment - related decisions for the franchise ##es who independently own and operate almost 100 percent of burger king restaurants . \" these franchise ##es do not work for the burger king company , and instead own their own burger king restaurants . they pay [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 2009 2001 3230 2011 2055 5539 2372 1997 1996 3435 1011 2833 14877 1010 4450 9209 1010 1998 3412 1998 3423 4177 1012 5971 3764 6491 2368 2005 9383 1005 1055 1998 15890 2332 2360 11897 11933 2003 10043 2000 2194 3343 1998 2008 9989 2024 10847 1012 1001 1001 2043 11897 11933 5158 15890 2332 2056 9432 2008 1996 2194 1000 17475 1996 2916 1997 2035 3667 1012 1000 1996 2194 2056 2009 1000 2515 2025 2191 19940 1010 11897 2030 2060 6107 1011 3141 6567 2005 1996 6329 2229 2040 9174 2219 1998 5452 2471 2531 3867 1997 15890 2332 7884 1012 1000 2122 6329 2229 2079 2025 2147 2005 1996 15890 2332 2194 1010 1998 2612 2219 2037 2219 15890 2332 7884 1012 2027 3477 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 2009 2001 3230 2011 2055 5539 2372 1997 1996 3435 1011 2833 14877 1010 4450 9209 1010 1998 3412 1998 3423 4177 1012 5971 3764 6491 2368 2005 9383 1005 1055 1998 15890 2332 2360 11897 11933 2003 10043 2000 2194 3343 1998 2008 9989 2024 10847 1012 1001 1001 2043 11897 11933 5158 15890 2332 2056 9432 2008 1996 2194 1000 17475 1996 2916 1997 2035 3667 1012 1000 1996 2194 2056 2009 1000 2515 2025 2191 19940 1010 11897 2030 2060 6107 1011 3141 6567 2005 1996 6329 2229 2040 9174 2219 1998 5452 2471 2531 3867 1997 15890 2332 7884 1012 1000 2122 6329 2229 2079 2025 2147 2005 1996 15890 2332 2194 1010 1998 2612 2219 2037 2219 15890 2332 7884 1012 2027 3477 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? has the disease developed new forms ? will the number of cases in seoul continue to rise ? south korean scientists ke ##e - jong hong and sung - han kim met with journalists to discuss what is known about the disease and its spread . there is no known medicine to fight off the disease , they said . up to four out of every 10 people who catch it die . still , the two scientists did have some good news . it is not clear how mer ##s spreads among people , the scientists said . however , it is common for many diseases to mu ##tate — to develop into new forms . once they mu ##tate , diseases can become much easier to [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? has the disease developed new forms ? will the number of cases in seoul continue to rise ? south korean scientists ke ##e - jong hong and sung - han kim met with journalists to discuss what is known about the disease and its spread . there is no known medicine to fight off the disease , they said . up to four out of every 10 people who catch it die . still , the two scientists did have some good news . it is not clear how mer ##s spreads among people , the scientists said . however , it is common for many diseases to mu ##tate — to develop into new forms . once they mu ##tate , diseases can become much easier to [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 2038 1996 4295 2764 2047 3596 1029 2097 1996 2193 1997 3572 1999 10884 3613 2000 4125 1029 2148 4759 6529 17710 2063 1011 18528 4291 1998 7042 1011 7658 5035 2777 2007 8845 2000 6848 2054 2003 2124 2055 1996 4295 1998 2049 3659 1012 2045 2003 2053 2124 4200 2000 2954 2125 1996 4295 1010 2027 2056 1012 2039 2000 2176 2041 1997 2296 2184 2111 2040 4608 2009 3280 1012 2145 1010 1996 2048 6529 2106 2031 2070 2204 2739 1012 2009 2003 2025 3154 2129 21442 2015 20861 2426 2111 1010 1996 6529 2056 1012 2174 1010 2009 2003 2691 2005 2116 7870 2000 14163 12259 1517 2000 4503 2046 2047 3596 1012 2320 2027 14163 12259 1010 7870 2064 2468 2172 6082 2000 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 2038 1996 4295 2764 2047 3596 1029 2097 1996 2193 1997 3572 1999 10884 3613 2000 4125 1029 2148 4759 6529 17710 2063 1011 18528 4291 1998 7042 1011 7658 5035 2777 2007 8845 2000 6848 2054 2003 2124 2055 1996 4295 1998 2049 3659 1012 2045 2003 2053 2124 4200 2000 2954 2125 1996 4295 1010 2027 2056 1012 2039 2000 2176 2041 1997 2296 2184 2111 2040 4608 2009 3280 1012 2145 1010 1996 2048 6529 2106 2031 2070 2204 2739 1012 2009 2003 2025 3154 2129 21442 2015 20861 2426 2111 1010 1996 6529 2056 1012 2174 1010 2009 2003 2691 2005 2116 7870 2000 14163 12259 1517 2000 4503 2046 2047 3596 1012 2320 2027 14163 12259 1010 7870 2064 2468 2172 6082 2000 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he had nervously put it off , staying up late with thick practice books and monster energy drinks . he wanted to go further than the parents who abandoned him . his father was in prison . his mother met a man , he says , and left him with his stepfather . then his stepfather lost his job , got evicted and crashed on a friend ' s couch . and his aunt , his third temporary guardian in three years , ran off with her boyfriend . his younger siblings , 5 , 7 and 11 , landed in a separate foster home . life turned around when mc ##ker ##nan met william child ##ress , 36 , a social worker who mentor ##s former foster kids [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he had nervously put it off , staying up late with thick practice books and monster energy drinks . he wanted to go further than the parents who abandoned him . his father was in prison . his mother met a man , he says , and left him with his stepfather . then his stepfather lost his job , got evicted and crashed on a friend ' s couch . and his aunt , his third temporary guardian in three years , ran off with her boyfriend . his younger siblings , 5 , 7 and 11 , landed in a separate foster home . life turned around when mc ##ker ##nan met william child ##ress , 36 , a social worker who mentor ##s former foster kids [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 2018 12531 2404 2009 2125 1010 6595 2039 2397 2007 4317 3218 2808 1998 6071 2943 8974 1012 2002 2359 2000 2175 2582 2084 1996 3008 2040 4704 2032 1012 2010 2269 2001 1999 3827 1012 2010 2388 2777 1037 2158 1010 2002 2758 1010 1998 2187 2032 2007 2010 21481 1012 2059 2010 21481 2439 2010 3105 1010 2288 25777 1998 8007 2006 1037 2767 1005 1055 6411 1012 1998 2010 5916 1010 2010 2353 5741 6697 1999 2093 2086 1010 2743 2125 2007 2014 6898 1012 2010 3920 9504 1010 1019 1010 1021 1998 2340 1010 5565 1999 1037 3584 6469 2188 1012 2166 2357 2105 2043 11338 5484 7229 2777 2520 2775 8303 1010 4029 1010 1037 2591 7309 2040 10779 2015 2280 6469 4268 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 2018 12531 2404 2009 2125 1010 6595 2039 2397 2007 4317 3218 2808 1998 6071 2943 8974 1012 2002 2359 2000 2175 2582 2084 1996 3008 2040 4704 2032 1012 2010 2269 2001 1999 3827 1012 2010 2388 2777 1037 2158 1010 2002 2758 1010 1998 2187 2032 2007 2010 21481 1012 2059 2010 21481 2439 2010 3105 1010 2288 25777 1998 8007 2006 1037 2767 1005 1055 6411 1012 1998 2010 5916 1010 2010 2353 5741 6697 1999 2093 2086 1010 2743 2125 2007 2014 6898 1012 2010 3920 9504 1010 1019 1010 1021 1998 2340 1010 5565 1999 1037 3584 6469 2188 1012 2166 2357 2105 2043 11338 5484 7229 2777 2520 2775 8303 1010 4029 1010 1037 2591 7309 2040 10779 2015 2280 6469 4268 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that dim ##ini ##sh their lives and cost our economy hundreds of billions of dollars a year . one part of obama ' s overall program is the healthy , hunger - free kids act . it ' s an update to the national school lunch program , which has helped pay for school meals since 1946 . more than 30 million students now participate , but the program hadn ' t had a major overhaul in 15 years . # # making school meals health ##ier following recommendations of the institute of medicine , school meals are now supposed to contain fewer cal ##ories , less fat and salt , and more fruit , vegetables and whole grains . most parents would agree these are ad ##mir ##able [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that dim ##ini ##sh their lives and cost our economy hundreds of billions of dollars a year . one part of obama ' s overall program is the healthy , hunger - free kids act . it ' s an update to the national school lunch program , which has helped pay for school meals since 1946 . more than 30 million students now participate , but the program hadn ' t had a major overhaul in 15 years . # # making school meals health ##ier following recommendations of the institute of medicine , school meals are now supposed to contain fewer cal ##ories , less fat and salt , and more fruit , vegetables and whole grains . most parents would agree these are ad ##mir ##able [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 11737 5498 4095 2037 3268 1998 3465 2256 4610 5606 1997 25501 1997 6363 1037 2095 1012 2028 2112 1997 8112 1005 1055 3452 2565 2003 1996 7965 1010 9012 1011 2489 4268 2552 1012 2009 1005 1055 2019 10651 2000 1996 2120 2082 6265 2565 1010 2029 2038 3271 3477 2005 2082 12278 2144 3918 1012 2062 2084 2382 2454 2493 2085 5589 1010 2021 1996 2565 2910 1005 1056 2018 1037 2350 18181 1999 2321 2086 1012 1001 1001 2437 2082 12278 2740 3771 2206 11433 1997 1996 2820 1997 4200 1010 2082 12278 2024 2085 4011 2000 5383 8491 10250 18909 1010 2625 6638 1998 5474 1010 1998 2062 5909 1010 11546 1998 2878 17588 1012 2087 3008 2052 5993 2122 2024 4748 14503 3085 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 11737 5498 4095 2037 3268 1998 3465 2256 4610 5606 1997 25501 1997 6363 1037 2095 1012 2028 2112 1997 8112 1005 1055 3452 2565 2003 1996 7965 1010 9012 1011 2489 4268 2552 1012 2009 1005 1055 2019 10651 2000 1996 2120 2082 6265 2565 1010 2029 2038 3271 3477 2005 2082 12278 2144 3918 1012 2062 2084 2382 2454 2493 2085 5589 1010 2021 1996 2565 2910 1005 1056 2018 1037 2350 18181 1999 2321 2086 1012 1001 1001 2437 2082 12278 2740 3771 2206 11433 1997 1996 2820 1997 4200 1010 2082 12278 2024 2085 4011 2000 5383 8491 10250 18909 1010 2625 6638 1998 5474 1010 1998 2062 5909 1010 11546 1998 2878 17588 1012 2087 3008 2052 5993 2122 2024 4748 14503 3085 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for our data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        \"\"\"\n",
        "        Removed \n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        \"\"\"\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            # \"f1_score\": f1_score,\n",
        "            # \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### These hyperparameters are recommended by the authors for fine-tuning BERT ###\n",
        "\n",
        "# Compute train and warmup steps from batch size\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 3e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "outputId": "3829cf0e-f53e-4383-e4ba-f93f304d410b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "print('Number of training steps: %d' % num_train_steps)\n",
        "print('Number of warmup steps: %d' % num_warmup_steps)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training steps: 803\n",
            "Number of warmup steps: 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "ecb25317-0156-489a-b1ee-5b7c3fc01227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpshrwpsdo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpshrwpsdo\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpshrwpsdo', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9ba3db4ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpshrwpsdo', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9ba3db4ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Finish setting up. Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "4d8501be-a2ce-4f79-d45d-dac918509dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-da2ac332234a>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-da2ac332234a>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2200: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2200: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.5842631, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.5842631, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.932543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.932543\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6759875, step = 100 (107.238 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6759875, step = 100 (107.238 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08055\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.5265092, step = 200 (92.542 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.5265092, step = 200 (92.542 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08122\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26013952, step = 300 (92.488 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26013952, step = 300 (92.488 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.08011\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3313226, step = 400 (92.583 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3313226, step = 400 (92.583 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.01549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.01549\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.43073148, step = 500 (98.477 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.43073148, step = 500 (98.477 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07919\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.10688854, step = 600 (92.660 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.10688854, step = 600 (92.660 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07972\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.33154035, step = 700 (92.620 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.33154035, step = 700 (92.620 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.07922\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.13702622, step = 800 (92.661 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.13702622, step = 800 (92.661 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 803 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 803 into /tmp/tmpshrwpsdo/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.13265519.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.13265519.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:13:34.596266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Evaluation with our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "outputId": "e4dbd20a-e542-4c3e-adf4-d8a06b68ddfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-12-20T03:45:16Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-12-20T03:45:16Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-12-20-03:45:31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-12-20-03:45:31\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 803: eval_accuracy = 0.8121721, false_negatives = 11.0, false_positives = 20.0, global_step = 803, loss = 0.54413605, precision = 0.97405964, recall = 0.9855643, true_negatives = 171.0, true_positives = 751.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 803: eval_accuracy = 0.8121721, false_negatives = 11.0, false_positives = 20.0, global_step = 803, loss = 0.54413605, precision = 0.97405964, recall = 0.9855643, true_negatives = 171.0, true_positives = 751.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 803: /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 803: /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.8121721,\n",
              " 'false_negatives': 11.0,\n",
              " 'false_positives': 20.0,\n",
              " 'global_step': 803,\n",
              " 'loss': 0.54413605,\n",
              " 'precision': 0.97405964,\n",
              " 'recall': 0.9855643,\n",
              " 'true_negatives': 171.0,\n",
              " 'true_positives': 751.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Now let's examine some of the examples that we got wrong:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQJTztEUCmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wrong_examples(test_set):\n",
        "  input_examples = test_set.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis=1)\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  \n",
        "  pred_labels = [pred['labels'] for pred in list(predictions)]\n",
        "\n",
        "  wrong_out = [] \n",
        "  for i in range(len(test_set)):\n",
        "    pred_label = pred_labels[i]\n",
        "    gt_label = test_set.iloc[i].label\n",
        "    if pred_label != gt_label:\n",
        "      wrong_out.append([gt_label, pred_label, test_set.iloc[i].text])\n",
        "  \n",
        "  return wrong_out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BflJRmiiUMZS",
        "colab_type": "code",
        "outputId": "2ca67238-75ca-4803-b2bc-dc59edba076e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "wrong_examples = get_wrong_examples(test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 953\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take the test pass . and that ' s not just bad luck at a single factory or in a single industry . hahn , vice president of organizational development , said that the poor scores on his company ' s math test have been evident for the past six years . he also sits on an aerospace workforce training committee and said that most other washington state suppliers in his industry have been seeing the same problem . \" you could think that even for production , do you really need to know math ? \" said jace ##y wilkins , a spoke ##sw ##oman for the manufacturing institute , an affiliate of the national association of manufacturers . \" but the truth is , you do , [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] washington — before job seekers fill out an application for work making foam products for the aerospace industry at general plastics manufacturing co . in tacoma , washington , they have to take a math test . eighteen questions , 30 minutes , and using a cal ##cula ##tor is ok . they are asked how to convert inches to feet , read a tape measure and find the density of a block of foam ( mass divided by volume ) . basic middle school math , right ? it ' s supposed to be . but what troubles general plastics executive eric hahn is that although the company considers only prospective workers who have a high school education , only 1 in 10 who take the test pass . and that ' s not just bad luck at a single factory or in a single industry . hahn , vice president of organizational development , said that the poor scores on his company ' s math test have been evident for the past six years . he also sits on an aerospace workforce training committee and said that most other washington state suppliers in his industry have been seeing the same problem . \" you could think that even for production , do you really need to know math ? \" said jace ##y wilkins , a spoke ##sw ##oman for the manufacturing institute , an affiliate of the national association of manufacturers . \" but the truth is , you do , [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 1996 3231 3413 1012 1998 2008 1005 1055 2025 2074 2919 6735 2012 1037 2309 4713 2030 1999 1037 2309 3068 1012 24266 1010 3580 2343 1997 13296 2458 1010 2056 2008 1996 3532 7644 2006 2010 2194 1005 1055 8785 3231 2031 2042 10358 2005 1996 2627 2416 2086 1012 2002 2036 7719 2006 2019 13395 14877 2731 2837 1998 2056 2008 2087 2060 2899 2110 20141 1999 2010 3068 2031 2042 3773 1996 2168 3291 1012 1000 2017 2071 2228 2008 2130 2005 2537 1010 2079 2017 2428 2342 2000 2113 8785 1029 1000 2056 10352 2100 22745 1010 1037 3764 26760 20778 2005 1996 5814 2820 1010 2019 8727 1997 1996 2120 2523 1997 8712 1012 1000 2021 1996 3606 2003 1010 2017 2079 1010 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2899 1517 2077 3105 24071 6039 2041 2019 4646 2005 2147 2437 17952 3688 2005 1996 13395 3068 2012 2236 26166 5814 2522 1012 1999 22954 1010 2899 1010 2027 2031 2000 2202 1037 8785 3231 1012 7763 3980 1010 2382 2781 1010 1998 2478 1037 10250 19879 4263 2003 7929 1012 2027 2024 2356 2129 2000 10463 5282 2000 2519 1010 3191 1037 6823 5468 1998 2424 1996 4304 1997 1037 3796 1997 17952 1006 3742 4055 2011 3872 1007 1012 3937 2690 2082 8785 1010 2157 1029 2009 1005 1055 4011 2000 2022 1012 2021 2054 13460 2236 26166 3237 4388 24266 2003 2008 2348 1996 2194 10592 2069 17464 3667 2040 2031 1037 2152 2082 2495 1010 2069 1015 1999 2184 2040 2202 1996 3231 3413 1012 1998 2008 1005 1055 2025 2074 2919 6735 2012 1037 2309 4713 2030 1999 1037 2309 3068 1012 24266 1010 3580 2343 1997 13296 2458 1010 2056 2008 1996 3532 7644 2006 2010 2194 1005 1055 8785 3231 2031 2042 10358 2005 1996 2627 2416 2086 1012 2002 2036 7719 2006 2019 13395 14877 2731 2837 1998 2056 2008 2087 2060 2899 2110 20141 1999 2010 3068 2031 2042 3773 1996 2168 3291 1012 1000 2017 2071 2228 2008 2130 2005 2537 1010 2079 2017 2428 2342 2000 2113 8785 1029 1000 2056 10352 2100 22745 1010 1037 3764 26760 20778 2005 1996 5814 2820 1010 2019 8727 1997 1996 2120 2523 1997 8712 1012 1000 2021 1996 3606 2003 1010 2017 2079 1010 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . it was attended by about 250 members of the fast - food workforce , labor unions , and religious and legal leaders . corporate spoke ##sm ##en for mcdonald ' s and burger king say wage theft is contrary to company policy and that allegations are investigated . # # when wage theft occurs burger king said thursday that the company \" respects the rights of all workers . \" the company said it \" does not make scheduling , wage or other employment - related decisions for the franchise ##es who independently own and operate almost 100 percent of burger king restaurants . \" these franchise ##es do not work for the burger king company , and instead own their own burger king restaurants . they pay [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] a new issue is growing out of the drive for a higher minimum wage for fast - food employees : wage theft , a term for failing to pay workers what they ' re legally owed . in recent months , lawsuits charging wage theft abuses have been filed on behalf of fast - food workers in three states . public attorneys in some states have obtained he ##ft ##y settlements from employers charged with violations . the issue came to the forefront thursday in front of three mcdonald ' s and burger king restaurants in kansas city , mo . signs there pro ##claiming \" wage theft \" and \" stolen wages \" dotted a midday rally by the stand ##up ##k ##c coalition . it was attended by about 250 members of the fast - food workforce , labor unions , and religious and legal leaders . corporate spoke ##sm ##en for mcdonald ' s and burger king say wage theft is contrary to company policy and that allegations are investigated . # # when wage theft occurs burger king said thursday that the company \" respects the rights of all workers . \" the company said it \" does not make scheduling , wage or other employment - related decisions for the franchise ##es who independently own and operate almost 100 percent of burger king restaurants . \" these franchise ##es do not work for the burger king company , and instead own their own burger king restaurants . they pay [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 2009 2001 3230 2011 2055 5539 2372 1997 1996 3435 1011 2833 14877 1010 4450 9209 1010 1998 3412 1998 3423 4177 1012 5971 3764 6491 2368 2005 9383 1005 1055 1998 15890 2332 2360 11897 11933 2003 10043 2000 2194 3343 1998 2008 9989 2024 10847 1012 1001 1001 2043 11897 11933 5158 15890 2332 2056 9432 2008 1996 2194 1000 17475 1996 2916 1997 2035 3667 1012 1000 1996 2194 2056 2009 1000 2515 2025 2191 19940 1010 11897 2030 2060 6107 1011 3141 6567 2005 1996 6329 2229 2040 9174 2219 1998 5452 2471 2531 3867 1997 15890 2332 7884 1012 1000 2122 6329 2229 2079 2025 2147 2005 1996 15890 2332 2194 1010 1998 2612 2219 2037 2219 15890 2332 7884 1012 2027 3477 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1037 2047 3277 2003 3652 2041 1997 1996 3298 2005 1037 3020 6263 11897 2005 3435 1011 2833 5126 1024 11897 11933 1010 1037 2744 2005 7989 2000 3477 3667 2054 2027 1005 2128 10142 12232 1012 1999 3522 2706 1010 20543 13003 11897 11933 21078 2031 2042 6406 2006 6852 1997 3435 1011 2833 3667 1999 2093 2163 1012 2270 16214 1999 2070 2163 2031 4663 2002 6199 2100 7617 2013 12433 5338 2007 13302 1012 1996 3277 2234 2000 1996 22870 9432 1999 2392 1997 2093 9383 1005 1055 1998 15890 2332 7884 1999 5111 2103 1010 9587 1012 5751 2045 4013 27640 1000 11897 11933 1000 1998 1000 7376 12678 1000 20384 1037 22878 8320 2011 1996 3233 6279 2243 2278 6056 1012 2009 2001 3230 2011 2055 5539 2372 1997 1996 3435 1011 2833 14877 1010 4450 9209 1010 1998 3412 1998 3423 4177 1012 5971 3764 6491 2368 2005 9383 1005 1055 1998 15890 2332 2360 11897 11933 2003 10043 2000 2194 3343 1998 2008 9989 2024 10847 1012 1001 1001 2043 11897 11933 5158 15890 2332 2056 9432 2008 1996 2194 1000 17475 1996 2916 1997 2035 3667 1012 1000 1996 2194 2056 2009 1000 2515 2025 2191 19940 1010 11897 2030 2060 6107 1011 3141 6567 2005 1996 6329 2229 2040 9174 2219 1998 5452 2471 2531 3867 1997 15890 2332 7884 1012 1000 2122 6329 2229 2079 2025 2147 2005 1996 15890 2332 2194 1010 1998 2612 2219 2037 2219 15890 2332 7884 1012 2027 3477 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? has the disease developed new forms ? will the number of cases in seoul continue to rise ? south korean scientists ke ##e - jong hong and sung - han kim met with journalists to discuss what is known about the disease and its spread . there is no known medicine to fight off the disease , they said . up to four out of every 10 people who catch it die . still , the two scientists did have some good news . it is not clear how mer ##s spreads among people , the scientists said . however , it is common for many diseases to mu ##tate — to develop into new forms . once they mu ##tate , diseases can become much easier to [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] seoul , south korea — many people in south korea ' s capital are wearing face ##mas ##ks this week . they are trying to protect themselves from a deadly disease : middle east respiratory syndrome ( mer ##s ) . an outbreak of the disease began here in may . a man who caught mer ##s in the middle east brought it with him when he returned to seoul . # # 108 cases , lots of questions so far , south korea is the only country outside the middle east to have a mer ##s outbreak . the country now has 108 cases of mer ##s . many questions — and fears — surround the disease . how easy is it to catch ? has the disease developed new forms ? will the number of cases in seoul continue to rise ? south korean scientists ke ##e - jong hong and sung - han kim met with journalists to discuss what is known about the disease and its spread . there is no known medicine to fight off the disease , they said . up to four out of every 10 people who catch it die . still , the two scientists did have some good news . it is not clear how mer ##s spreads among people , the scientists said . however , it is common for many diseases to mu ##tate — to develop into new forms . once they mu ##tate , diseases can become much easier to [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 2038 1996 4295 2764 2047 3596 1029 2097 1996 2193 1997 3572 1999 10884 3613 2000 4125 1029 2148 4759 6529 17710 2063 1011 18528 4291 1998 7042 1011 7658 5035 2777 2007 8845 2000 6848 2054 2003 2124 2055 1996 4295 1998 2049 3659 1012 2045 2003 2053 2124 4200 2000 2954 2125 1996 4295 1010 2027 2056 1012 2039 2000 2176 2041 1997 2296 2184 2111 2040 4608 2009 3280 1012 2145 1010 1996 2048 6529 2106 2031 2070 2204 2739 1012 2009 2003 2025 3154 2129 21442 2015 20861 2426 2111 1010 1996 6529 2056 1012 2174 1010 2009 2003 2691 2005 2116 7870 2000 14163 12259 1517 2000 4503 2046 2047 3596 1012 2320 2027 14163 12259 1010 7870 2064 2468 2172 6082 2000 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 10884 1010 2148 4420 1517 2116 2111 1999 2148 4420 1005 1055 3007 2024 4147 2227 9335 5705 2023 2733 1012 2027 2024 2667 2000 4047 3209 2013 1037 9252 4295 1024 2690 2264 16464 8715 1006 21442 2015 1007 1012 2019 8293 1997 1996 4295 2211 2182 1999 2089 1012 1037 2158 2040 3236 21442 2015 1999 1996 2690 2264 2716 2009 2007 2032 2043 2002 2513 2000 10884 1012 1001 1001 10715 3572 1010 7167 1997 3980 2061 2521 1010 2148 4420 2003 1996 2069 2406 2648 1996 2690 2264 2000 2031 1037 21442 2015 8293 1012 1996 2406 2085 2038 10715 3572 1997 21442 2015 1012 2116 3980 1517 1998 10069 1517 15161 1996 4295 1012 2129 3733 2003 2009 2000 4608 1029 2038 1996 4295 2764 2047 3596 1029 2097 1996 2193 1997 3572 1999 10884 3613 2000 4125 1029 2148 4759 6529 17710 2063 1011 18528 4291 1998 7042 1011 7658 5035 2777 2007 8845 2000 6848 2054 2003 2124 2055 1996 4295 1998 2049 3659 1012 2045 2003 2053 2124 4200 2000 2954 2125 1996 4295 1010 2027 2056 1012 2039 2000 2176 2041 1997 2296 2184 2111 2040 4608 2009 3280 1012 2145 1010 1996 2048 6529 2106 2031 2070 2204 2739 1012 2009 2003 2025 3154 2129 21442 2015 20861 2426 2111 1010 1996 6529 2056 1012 2174 1010 2009 2003 2691 2005 2116 7870 2000 14163 12259 1517 2000 4503 2046 2047 3596 1012 2320 2027 14163 12259 1010 7870 2064 2468 2172 6082 2000 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he had nervously put it off , staying up late with thick practice books and monster energy drinks . he wanted to go further than the parents who abandoned him . his father was in prison . his mother met a man , he says , and left him with his stepfather . then his stepfather lost his job , got evicted and crashed on a friend ' s couch . and his aunt , his third temporary guardian in three years , ran off with her boyfriend . his younger siblings , 5 , 7 and 11 , landed in a separate foster home . life turned around when mc ##ker ##nan met william child ##ress , 36 , a social worker who mentor ##s former foster kids [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] michael mc ##ker ##nan ' s heart pounded as he approached the community college in gall ##atin , tennessee , and the exam that would decide his future . he had recently turned 18 , which aged him out of foster care . mc ##ker ##nan , a lan ##ky jokes ##ter who speaks with a southern t ##wang , had dropped out of high school and needed to test for the ge ##d that day or risk losing a shot at state - funded scholarships for college . he knew he could not afford tuition as an overnight stock ##er at wal - mart . it was dec . 11 , the last opportunity that year to take the test in his county . he had nervously put it off , staying up late with thick practice books and monster energy drinks . he wanted to go further than the parents who abandoned him . his father was in prison . his mother met a man , he says , and left him with his stepfather . then his stepfather lost his job , got evicted and crashed on a friend ' s couch . and his aunt , his third temporary guardian in three years , ran off with her boyfriend . his younger siblings , 5 , 7 and 11 , landed in a separate foster home . life turned around when mc ##ker ##nan met william child ##ress , 36 , a social worker who mentor ##s former foster kids [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 2018 12531 2404 2009 2125 1010 6595 2039 2397 2007 4317 3218 2808 1998 6071 2943 8974 1012 2002 2359 2000 2175 2582 2084 1996 3008 2040 4704 2032 1012 2010 2269 2001 1999 3827 1012 2010 2388 2777 1037 2158 1010 2002 2758 1010 1998 2187 2032 2007 2010 21481 1012 2059 2010 21481 2439 2010 3105 1010 2288 25777 1998 8007 2006 1037 2767 1005 1055 6411 1012 1998 2010 5916 1010 2010 2353 5741 6697 1999 2093 2086 1010 2743 2125 2007 2014 6898 1012 2010 3920 9504 1010 1019 1010 1021 1998 2340 1010 5565 1999 1037 3584 6469 2188 1012 2166 2357 2105 2043 11338 5484 7229 2777 2520 2775 8303 1010 4029 1010 1037 2591 7309 2040 10779 2015 2280 6469 4268 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2745 11338 5484 7229 1005 1055 2540 13750 2004 2002 5411 1996 2451 2267 1999 26033 20363 1010 5298 1010 1998 1996 11360 2008 2052 5630 2010 2925 1012 2002 2018 3728 2357 2324 1010 2029 4793 2032 2041 1997 6469 2729 1012 11338 5484 7229 1010 1037 17595 4801 13198 3334 2040 8847 2007 1037 2670 1056 16600 1010 2018 3333 2041 1997 2152 2082 1998 2734 2000 3231 2005 1996 16216 2094 2008 2154 2030 3891 3974 1037 2915 2012 2110 1011 6787 15691 2005 2267 1012 2002 2354 2002 2071 2025 8984 15413 2004 2019 11585 4518 2121 2012 24547 1011 20481 1012 2009 2001 11703 1012 2340 1010 1996 2197 4495 2008 2095 2000 2202 1996 3231 1999 2010 2221 1012 2002 2018 12531 2404 2009 2125 1010 6595 2039 2397 2007 4317 3218 2808 1998 6071 2943 8974 1012 2002 2359 2000 2175 2582 2084 1996 3008 2040 4704 2032 1012 2010 2269 2001 1999 3827 1012 2010 2388 2777 1037 2158 1010 2002 2758 1010 1998 2187 2032 2007 2010 21481 1012 2059 2010 21481 2439 2010 3105 1010 2288 25777 1998 8007 2006 1037 2767 1005 1055 6411 1012 1998 2010 5916 1010 2010 2353 5741 6697 1999 2093 2086 1010 2743 2125 2007 2014 6898 1012 2010 3920 9504 1010 1019 1010 1021 1998 2340 1010 5565 1999 1037 3584 6469 2188 1012 2166 2357 2105 2043 11338 5484 7229 2777 2520 2775 8303 1010 4029 1010 1037 2591 7309 2040 10779 2015 2280 6469 4268 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that dim ##ini ##sh their lives and cost our economy hundreds of billions of dollars a year . one part of obama ' s overall program is the healthy , hunger - free kids act . it ' s an update to the national school lunch program , which has helped pay for school meals since 1946 . more than 30 million students now participate , but the program hadn ' t had a major overhaul in 15 years . # # making school meals health ##ier following recommendations of the institute of medicine , school meals are now supposed to contain fewer cal ##ories , less fat and salt , and more fruit , vegetables and whole grains . most parents would agree these are ad ##mir ##able [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # # # pro : fight efforts to water them down washington — as a new school year begins , american parents should enthusiastically join first lady michelle obama ' s campaign for health ##ier school lunch ##es . her drive is based on sound nutritional science with the goal of health ##ier , happier kids . the first lady has made improving childhood health through better eating and more exercise her signature issue . that ' s a wise choice , since childhood obesity reached epidemic proportions : in 2012 , 1 in 3 american children were over ##weight or obe ##se . over ##weight children are at higher risk of developing a variety of ai ##lm ##ents , including cardiovascular disease and diabetes that dim ##ini ##sh their lives and cost our economy hundreds of billions of dollars a year . one part of obama ' s overall program is the healthy , hunger - free kids act . it ' s an update to the national school lunch program , which has helped pay for school meals since 1946 . more than 30 million students now participate , but the program hadn ' t had a major overhaul in 15 years . # # making school meals health ##ier following recommendations of the institute of medicine , school meals are now supposed to contain fewer cal ##ories , less fat and salt , and more fruit , vegetables and whole grains . most parents would agree these are ad ##mir ##able [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 11737 5498 4095 2037 3268 1998 3465 2256 4610 5606 1997 25501 1997 6363 1037 2095 1012 2028 2112 1997 8112 1005 1055 3452 2565 2003 1996 7965 1010 9012 1011 2489 4268 2552 1012 2009 1005 1055 2019 10651 2000 1996 2120 2082 6265 2565 1010 2029 2038 3271 3477 2005 2082 12278 2144 3918 1012 2062 2084 2382 2454 2493 2085 5589 1010 2021 1996 2565 2910 1005 1056 2018 1037 2350 18181 1999 2321 2086 1012 1001 1001 2437 2082 12278 2740 3771 2206 11433 1997 1996 2820 1997 4200 1010 2082 12278 2024 2085 4011 2000 5383 8491 10250 18909 1010 2625 6638 1998 5474 1010 1998 2062 5909 1010 11546 1998 2878 17588 1012 2087 3008 2052 5993 2122 2024 4748 14503 3085 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 1001 1001 4013 1024 2954 4073 2000 2300 2068 2091 2899 1517 2004 1037 2047 2082 2095 4269 1010 2137 3008 2323 24935 3693 2034 3203 9393 8112 1005 1055 3049 2005 2740 3771 2082 6265 2229 1012 2014 3298 2003 2241 2006 2614 28268 2671 2007 1996 3125 1997 2740 3771 1010 19366 4268 1012 1996 2034 3203 2038 2081 9229 5593 2740 2083 2488 5983 1998 2062 6912 2014 8085 3277 1012 2008 1005 1055 1037 7968 3601 1010 2144 5593 24552 2584 16311 19173 1024 1999 2262 1010 1015 1999 1017 2137 2336 2020 2058 11179 2030 15578 3366 1012 2058 11179 2336 2024 2012 3020 3891 1997 4975 1037 3528 1997 9932 13728 11187 1010 2164 22935 4295 1998 14671 2008 11737 5498 4095 2037 3268 1998 3465 2256 4610 5606 1997 25501 1997 6363 1037 2095 1012 2028 2112 1997 8112 1005 1055 3452 2565 2003 1996 7965 1010 9012 1011 2489 4268 2552 1012 2009 1005 1055 2019 10651 2000 1996 2120 2082 6265 2565 1010 2029 2038 3271 3477 2005 2082 12278 2144 3918 1012 2062 2084 2382 2454 2493 2085 5589 1010 2021 1996 2565 2910 1005 1056 2018 1037 2350 18181 1999 2321 2086 1012 1001 1001 2437 2082 12278 2740 3771 2206 11433 1997 1996 2820 1997 4200 1010 2082 12278 2024 2085 4011 2000 5383 8491 10250 18909 1010 2625 6638 1998 5474 1010 1998 2062 5909 1010 11546 1998 2878 17588 1012 2087 3008 2052 5993 2122 2024 4748 14503 3085 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpshrwpsdo/model.ckpt-803\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va3-9IuDVLXH",
        "colab_type": "code",
        "outputId": "c446165a-6fe9-4618-a51b-8c94d47eae9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "more_than_two_lvl = 0\n",
        "text_len_avg = 0.0\n",
        "for gt_label, pred_label, text in wrong_examples:\n",
        "  text_len_avg += len(text.split())\n",
        "  if abs(gt_label > pred_label) > 1:\n",
        "    more_than_two_lvl += 1\n",
        "  print('Ground-truth label: {}. Predicted label: {}. Text length: {}'.format(gt_label, pred_label, len(text.split())))\n",
        "  # print(text)\n",
        "text_len_avg /= len(wrong_examples)\n",
        "\n",
        "print('BERT got {} examples wrong in total, in which {} were off by at least 2 levels.'.format(len(wrong_examples), more_than_two_lvl))\n",
        "print('Average length of misclassified text: {}'.format(text_len_avg))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground-truth label: 0. Predicted label: 1. Text length: 1578\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 943\n",
            "Ground-truth label: 0. Predicted label: 2. Text length: 1202\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 903\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 643\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 923\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 888\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 637\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 761\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 536\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 897\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 924\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 646\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 797\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 687\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 422\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 830\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 721\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 626\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 1459\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 866\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 692\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 657\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 826\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 663\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 588\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 588\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 554\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 795\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 818\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 804\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 1004\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 780\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 446\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 745\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 432\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 830\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 992\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 852\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 967\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 510\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 610\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 699\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 522\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 783\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 1562\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 560\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 658\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 765\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 574\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 885\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 801\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 637\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 631\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 831\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 572\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 627\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 1011\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 560\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 461\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 813\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 1361\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 769\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 862\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 879\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 852\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 757\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 1080\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 639\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 953\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 1382\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 929\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 479\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 709\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 674\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 663\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 539\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 1406\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 839\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 733\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 582\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 849\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 925\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 970\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 1315\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 746\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 749\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 1036\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 901\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 608\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 731\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 882\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 713\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 406\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 629\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 720\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 725\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 922\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 860\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 687\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 538\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 552\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 711\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 751\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 705\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 690\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 610\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 1358\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 608\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 561\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 834\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 876\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 620\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 939\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 1104\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 775\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 594\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 886\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 622\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 794\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 638\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 617\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 978\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 857\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 707\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 598\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 914\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 933\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 795\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 607\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 524\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 809\n",
            "Ground-truth label: 1. Predicted label: 0. Text length: 820\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 1306\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 884\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 881\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 853\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 566\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 607\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 620\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 839\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 606\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 610\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 763\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 882\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 615\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 831\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 519\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 544\n",
            "Ground-truth label: 1. Predicted label: 3. Text length: 771\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 852\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 821\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 619\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 669\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 689\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 577\n",
            "Ground-truth label: 2. Predicted label: 3. Text length: 598\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 662\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 665\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 623\n",
            "Ground-truth label: 3. Predicted label: 2. Text length: 752\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 938\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 716\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 976\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 691\n",
            "Ground-truth label: 4. Predicted label: 3. Text length: 562\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 469\n",
            "Ground-truth label: 0. Predicted label: 1. Text length: 710\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 619\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 767\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 621\n",
            "Ground-truth label: 1. Predicted label: 2. Text length: 738\n",
            "Ground-truth label: 3. Predicted label: 4. Text length: 639\n",
            "Ground-truth label: 2. Predicted label: 1. Text length: 599\n",
            "BERT got 174 examples wrong in total, in which 0 were off by at least 2 levels.\n",
            "Average length of misclassified text: 768.0977011494252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7JEjZW9aqzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}